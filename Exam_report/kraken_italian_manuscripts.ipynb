{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Italian Manuscripts OCR with Kraken\n",
    "\n",
    "This notebook documents the end-to-end process used to create and fine-tune a Kraken OCR model for Italian handwritten manuscripts. It follows the same logical structure of the LaTeX report:\n",
    "\n",
    "- Preparation of line images and Markdown transcriptions\n",
    "- Generation and normalization of the ground truth file\n",
    "- Creation of train/validation/test splits and charset\n",
    "- Image preprocessing pipeline\n",
    "- Creation of sidecar `.gt.txt` files\n",
    "- Final training command for Kraken via `ketos`\n",
    "\n",
    "The notebook is mainly **reproducibility and documentation**: some paths and commands are examples taken from the original environment and may need to be adapted to your own directory layout and platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment and prerequisites\n",
    "\n",
    "The work was carried out on a Linux machine (Pop!_OS) with an NVIDIA GPU and CUDA support. The following assumptions are made:\n",
    "\n",
    "- You have a working Python 3 environment (e.g. via `conda`).\n",
    "- `kraken` and `ketos` are installed in this environment (e.g. `pip install kraken`).\n",
    "- You have the following directory structure (or equivalent):\n",
    "  - `00_images/` – line images extracted from scanned pages\n",
    "  - `01_texts/` – Markdown files with manual transcriptions, one per line image\n",
    "  - `gt_new/` – folder where ground truth files will be written\n",
    "  - `splits_new/` – folder for train/val/test splits\n",
    "  - `processed/lines/` – folder for preprocessed line images used for training\n",
    "\n",
    "Below we reconstruct all the scripts and commands used in the LaTeX report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating ground truth from Markdown files\n",
    "\n",
    "We start from:\n",
    "- `00_images/` containing line images (e.g. `100_page1_line001.png`)\n",
    "- `01_texts/` containing Markdown transcriptions with matching stems (e.g. `100_page1_line001.md`)\n",
    "\n",
    "The script `make_gt_from_line_md.py` generates a tab-separated `ground_truth.txt` where each line contains:\n",
    "\n",
    "```text\n",
    "<image_path>\\t<normalized_plain_text>\n",
    "```\n",
    "\n",
    "Markdown syntax is stripped and multiple lines in the `.md` file can be concatenated into a single transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "bash"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Generate ground truth from Markdown transcriptions\n",
    "# Adjust paths as needed before running.\n",
    "python3 make_gt_from_line_md.py \\\n",
    "    --images_dir \"./00_images\" \\\n",
    "    --md_dir \"./01_texts\" \\\n",
    "    --out \"gt_new/ground_truth_new.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# make_gt_from_line_md.py\n",
    "#\n",
    "# Generates gt/ground_truth.txt associating each image in images_dir\n",
    "# with the corresponding markdown/text file in md_dir using the same\n",
    "# stem (e.g. 100_page1_line001.png -> 100_page1_line001.md).\n",
    "#\n",
    "# Usage example:\n",
    "# python3 make_gt_from_line_md.py --images_dir \"./00_images\" --md_dir \"./01_texts\" --out \"./gt/ground_truth.txt\"\n",
    "#\n",
    "# Options:\n",
    "# --images_glob : pattern for images (default '*.*')\n",
    "# --join_multiline : if true, concatenates all non-empty lines in .md (default True)\n",
    "# --relpath : if true, writes relative paths for images in the GT file\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import re\n",
    "import sys\n",
    "\n",
    "def strip_markdown(s: str) -> str:\n",
    "    \"\"\"Lightweight Markdown stripping: images, links, formatting, quotes, extra spaces.\"\"\"\n",
    "    s = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', s)    # images ![alt](url)\n",
    "    s = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', s)  # links [text](url) -> text\n",
    "    s = re.sub(r'[`*_]{1,}', '', s)             # backticks, asterisks, underscores\n",
    "    s = re.sub(r'^\\s*>\\s*', '', s, flags=re.M) # blockquotes\n",
    "    s = re.sub(r'\\s+', ' ', s)                 # collapse whitespace\n",
    "    return s.strip()\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    return unicodedata.normalize(\"NFC\", s).strip()\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument('--images_dir', required=True)\n",
    "    p.add_argument('--md_dir', required=True)\n",
    "    p.add_argument('--out', default='gt/ground_truth.txt')\n",
    "    p.add_argument('--images_glob', default='*.*', help=\"glob pattern for images, default '*.*'\")\n",
    "    p.add_argument('--join_multiline', action='store_true', default=True,\n",
    "                   help='Concatena più linee del .md in una')\n",
    "    p.add_argument('--no-join', dest='join_multiline', action='store_false',\n",
    "                   help='Non concatenare, usa solo la prima riga')\n",
    "    p.add_argument('--relpath', action='store_true', default=True,\n",
    "                   help='Usa path relative per le immagini nel GT')\n",
    "    args = p.parse_args()\n",
    "\n",
    "    images_dir = Path(args.images_dir)\n",
    "    md_dir = Path(args.md_dir)\n",
    "    outp = Path(args.out)\n",
    "    outp.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    imgs = sorted([p for p in images_dir.glob(args.images_glob) if p.is_file()])\n",
    "    if not imgs:\n",
    "        print(\"Nessuna immagine trovata in\", images_dir, file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    missing_md = []\n",
    "    written = 0\n",
    "    with outp.open('w', encoding='utf-8') as fh:\n",
    "        for img in imgs:\n",
    "            stem = img.stem\n",
    "            md_path = md_dir / (stem + '.md')\n",
    "            if not md_path.exists():\n",
    "                # Try other common text extensions\n",
    "                found = None\n",
    "                for ext in ['.txt', '.mdown', '.markdown']:\n",
    "                    cand = md_dir / (stem + ext)\n",
    "                    if cand.exists():\n",
    "                        found = cand\n",
    "                        break\n",
    "                if found:\n",
    "                    md_path = found\n",
    "                else:\n",
    "                    missing_md.append((img.name, str(md_path)))\n",
    "                    continue\n",
    "\n",
    "            text = md_path.read_text(encoding='utf-8')\n",
    "            lines = [normalize(strip_markdown(l)) for l in text.splitlines() if l.strip()]\n",
    "            if not lines:\n",
    "                # Empty transcription file\n",
    "                missing_md.append((img.name, f\"{md_path} (vuoto)\"))\n",
    "                continue\n",
    "\n",
    "            if args.join_multiline:\n",
    "                txt = ' '.join(lines)\n",
    "            else:\n",
    "                txt = lines[0]\n",
    "\n",
    "            txt = normalize(strip_markdown(txt))\n",
    "\n",
    "            # Image path: relative or absolute\n",
    "            if args.relpath:\n",
    "                img_path_str = str(Path(args.images_dir).joinpath(img.name).as_posix())\n",
    "            else:\n",
    "                img_path_str = str(img.resolve())\n",
    "\n",
    "            txt = txt.replace('\\t', ' ').replace('\\r', ' ').replace('\\n', ' ')\n",
    "            fh.write(f\"{img_path_str}\\t{txt}\\n\")\n",
    "            written += 1\n",
    "\n",
    "    print(f\"Wrote {written} lines to {outp}\")\n",
    "    if missing_md:\n",
    "        print(f\"Missing or empty md for {len(missing_md)} images (showing up to 20):\")\n",
    "        for m in missing_md[:20]:\n",
    "            print(\"  -\", m[0], \"expected\", m[1])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validation and normalization of ground truth\n",
    "\n",
    "The next step is to validate and normalize the generated `ground_truth_new.txt`:\n",
    "- Ensure each line has an image path and a transcription separated by a tab.\n",
    "- Normalize whitespace and Unicode (NFC).\n",
    "- Optionally apply substitutions (mapping file).\n",
    "- Check for missing image files and duplicates.\n",
    "\n",
    "The script `validate_and_normalize_gt.py` performs these checks and writes a normalized file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "bash"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Validate and normalize ground truth\n",
    "python3 validate_and_normalize_gt.py \\\n",
    "    --gt \"gt_new/ground_truth_new.txt\" \\\n",
    "    --root \".\" \\\n",
    "    --out \"gt_new/ground_truth_new_normalized.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# validate_and_normalize_gt.py\n",
    "#\n",
    "# Verifies and normalizes an existing ground_truth.txt.\n",
    "# Produces a cleaned version and a console report of problems found.\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_map(path: str):\n",
    "    m = {}\n",
    "    if not path:\n",
    "        return m\n",
    "    for ln in open(path, encoding='utf-8'):\n",
    "        ln = ln.rstrip('\\n')\n",
    "        if not ln or ln.startswith('#'):\n",
    "            continue\n",
    "        if '\\t' in ln:\n",
    "            a, b = ln.split('\\t', 1)\n",
    "        else:\n",
    "            parts = ln.split(None, 1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            a, b = parts\n",
    "        m[a] = b\n",
    "    return m\n",
    "\n",
    "def apply_map(s: str, mapping: dict) -> str:\n",
    "    for a, b in mapping.items():\n",
    "        s = s.replace(a, b)\n",
    "    return s\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = ' '.join(s.split())  # collapse whitespace\n",
    "    s = unicodedata.normalize('NFC', s)\n",
    "    return s\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument('--gt', required=True)\n",
    "    p.add_argument('--root', default='.', help='Root dir to resolve image paths')\n",
    "    p.add_argument('--out', default=None, help='Output normalized GT file (optional)')\n",
    "    p.add_argument('--map', default=None, help='Optional substitutions file (from<TAB>to)')\n",
    "    args = p.parse_args()\n",
    "\n",
    "    mapping = load_map(args.map)\n",
    "    root = Path(args.root)\n",
    "    gt = Path(args.gt)\n",
    "    if not gt.exists():\n",
    "        print('gt file not found', gt, file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    missing = []\n",
    "    empty = []\n",
    "    bad_lines = []\n",
    "    duplicates = defaultdict(int)\n",
    "    total = 0\n",
    "    normalized_lines = []\n",
    "\n",
    "    for i, ln in enumerate(gt.read_text(encoding='utf-8').splitlines(), start=1):\n",
    "        if not ln.strip():\n",
    "            continue\n",
    "        if '\\t' not in ln:\n",
    "            bad_lines.append((i, ln))\n",
    "            continue\n",
    "        path, txt = ln.split('\\t', 1)\n",
    "        path = path.strip()\n",
    "        txt = txt.strip()\n",
    "        txt = apply_map(txt, mapping)\n",
    "        txt = normalize_text(txt)\n",
    "        if txt == '':\n",
    "            empty.append((i, path))\n",
    "        pth = (root / path) if not Path(path).is_absolute() else Path(path)\n",
    "        if not pth.exists():\n",
    "            missing.append((i, path))\n",
    "        duplicates[path] += 1\n",
    "        normalized_lines.append((path, txt))\n",
    "        total += 1\n",
    "\n",
    "    print(f\"Total lines read: {total}\")\n",
    "    if bad_lines:\n",
    "        print(f\"Lines without tab: {len(bad_lines)} (showing up to 10):\")\n",
    "        for i, ln in bad_lines[:10]:\n",
    "            print(i, ln[:200])\n",
    "    if empty:\n",
    "        print(f\"Empty transcriptions: {len(empty)}\")\n",
    "    if missing:\n",
    "        print(f\"Missing image files: {len(missing)} (showing up to 10):\")\n",
    "        for i, pth in missing[:10]:\n",
    "            print(i, pth)\n",
    "    dup_list = [p for p, c in duplicates.items() if c > 1]\n",
    "    if dup_list:\n",
    "        print(f\"Duplicate image entries: {len(dup_list)} (showing up to 10):\")\n",
    "        for pth in dup_list[:10]:\n",
    "            print(pth, duplicates[pth])\n",
    "\n",
    "    if args.out:\n",
    "        outp = Path(args.out)\n",
    "        outp.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with outp.open('w', encoding='utf-8') as fh:\n",
    "            for path, txt in normalized_lines:\n",
    "                txt_clean = txt.replace('\\t', ' ').replace('\\r', ' ').replace('\\n', ' ')\n",
    "                fh.write(f\"{path}\\t{txt_clean}\\n\")\n",
    "        print(f\"Wrote normalized ground truth to {outp}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating splits and charset\n",
    "\n",
    "Once the normalized ground truth is available, we:\n",
    "- Group entries by page (to avoid page leakage between splits).\n",
    "- Split into train/validation/test sets.\n",
    "- Extract the charset (set of characters actually used).\n",
    "\n",
    "This is done by `split_and_charset.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "bash"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create train/val/test splits and charset\n",
    "python3 split_and_charset.py \\\n",
    "    gt_new/ground_truth_new_normalized.txt \\\n",
    "    --out splits_new \\\n",
    "    --val 0.05 \\\n",
    "    --test 0.05 \\\n",
    "    --group-by-page \\\n",
    "    --page-sep \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# split_and_charset.py\n",
    "#\n",
    "# Reads a ground truth file (path<TAB>transcription per line),\n",
    "# normalizes text, extracts the charset, and creates train/val/test splits.\n",
    "\n",
    "import argparse\n",
    "import unicodedata\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    return unicodedata.normalize(\"NFC\", s).strip()\n",
    "\n",
    "def read_ground_truth(gt_path: str):\n",
    "    entries = []\n",
    "    gt_path = Path(gt_path)\n",
    "    if not gt_path.exists():\n",
    "        raise FileNotFoundError(f\"Ground truth file not found: {gt_path}\")\n",
    "    with gt_path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        for lineno, line in enumerate(fh, 1):\n",
    "            raw = line.rstrip(\"\\n\")\n",
    "            if not raw.strip():\n",
    "                continue\n",
    "            if \"\\t\" in raw:\n",
    "                img, txt = raw.split(\"\\t\", 1)\n",
    "            else:\n",
    "                parts = raw.split(None, 1)\n",
    "                if len(parts) != 2:\n",
    "                    print(f\"Warning: salto linea {lineno} (formato non riconosciuto): {raw}\")\n",
    "                    continue\n",
    "                img, txt = parts\n",
    "            txt = normalize_text(txt)\n",
    "            if txt == \"\":\n",
    "                print(f\"Warning: trascrizione vuota a linea {lineno}, salto.\")\n",
    "                continue\n",
    "            entries.append((img, txt))\n",
    "    return entries\n",
    "\n",
    "def group_entries(entries, group_by_page: bool, page_sep: str):\n",
    "    if not group_by_page:\n",
    "        return [[e] for e in entries]\n",
    "    groups = defaultdict(list)\n",
    "    for img, txt in entries:\n",
    "        stem = Path(img).stem\n",
    "        if page_sep and page_sep in stem:\n",
    "            key = stem.split(page_sep, 1)[0]\n",
    "        else:\n",
    "            parent = str(Path(img).parent)\n",
    "            key = parent\n",
    "        groups[key].append((img, txt))\n",
    "    return list(groups.values())\n",
    "\n",
    "def split_groups(groups, val_frac: float, test_frac: float, seed: int):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(groups)\n",
    "    total = sum(len(g) for g in groups)\n",
    "    n_val = int(total * val_frac + 0.5)\n",
    "    n_test = int(total * test_frac + 0.5)\n",
    "    n_train = total - n_val - n_test\n",
    "\n",
    "    train, val, test = [], [], []\n",
    "    counts = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "\n",
    "    for g in groups:\n",
    "        deficits = {\n",
    "            \"train\": n_train - counts[\"train\"],\n",
    "            \"val\": n_val - counts[\"val\"],\n",
    "            \"test\": n_test - counts[\"test\"]\n",
    "        }\n",
    "        pick = max(deficits.items(), key=lambda x: (x[1], x[0]))[0]\n",
    "        if pick == \"train\":\n",
    "            train.extend(g); counts[\"train\"] += len(g)\n",
    "        elif pick == \"val\":\n",
    "            if counts[\"val\"] + len(g) <= n_val:\n",
    "                val.extend(g); counts[\"val\"] += len(g)\n",
    "            else:\n",
    "                train.extend(g); counts[\"train\"] += len(g)\n",
    "        else:\n",
    "            if counts[\"test\"] + len(g) <= n_test:\n",
    "                test.extend(g); counts[\"test\"] += len(g)\n",
    "            else:\n",
    "                train.extend(g); counts[\"train\"] += len(g)\n",
    "    return train, val, test\n",
    "\n",
    "def write_split(out_dir: str, name: str, entries):\n",
    "    out = Path(out_dir)\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    path = out / f\"{name}.txt\"\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        for img, txt in entries:\n",
    "            fh.write(f\"{img}\\t{txt}\\n\")\n",
    "    print(f\"Wrote {len(entries)} lines to {path}\")\n",
    "\n",
    "def extract_charset(entries, out_dir: str):\n",
    "    chars = set()\n",
    "    for _, txt in entries:\n",
    "        chars.update(txt)\n",
    "    chars.discard(\"\\n\"); chars.discard(\"\\r\")\n",
    "    chars = sorted(chars)\n",
    "    out = Path(out_dir); out.mkdir(parents=True, exist_ok=True)\n",
    "    charset_path = out / \"charset.txt\"\n",
    "    with charset_path.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        for c in chars:\n",
    "            fh.write(c + \"\\n\")\n",
    "    with (out / \"charset_one_line.txt\").open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        fh.write(\"\".join(chars))\n",
    "    print(f\"Extracted {len(chars)} unique characters -> {charset_path}\")\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser(description=\"Genera splits e charset da ground_truth\")\n",
    "    p.add_argument(\"gt\", help=\"Percorso al file ground_truth (path<TAB>trascrizione per riga)\")\n",
    "    p.add_argument(\"--out\", \"-o\", default=\"splits\", help=\"Cartella di output\")\n",
    "    p.add_argument(\"--val\", type=float, default=0.05, help=\"Frazione per validation (default 0.05)\")\n",
    "    p.add_argument(\"--test\", type=float, default=0.05, help=\"Frazione per test (default 0.05)\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42, help=\"Seed per random shuffle\")\n",
    "    p.add_argument(\"--group-by-page\", action=\"store_true\",\n",
    "                   help=\"Evitare contaminazione raggruppando righe della stessa pagina\")\n",
    "    p.add_argument(\"--page-sep\", default=\"_\",\n",
    "                   help=\"Separatore per estrarre ID pagina dal file stem\")\n",
    "    args = p.parse_args()\n",
    "\n",
    "    entries = read_ground_truth(args.gt)\n",
    "    if not entries:\n",
    "        print(\"Nessuna entry trovata nel ground truth. Esco.\")\n",
    "        return\n",
    "    print(f\"Letti {len(entries)} righe dal ground truth.\")\n",
    "    groups = group_entries(entries, args.group_by_page, args.page_sep)\n",
    "    print(f\"Raggruppati in {len(groups)} gruppi (group_by_page={args.group_by_page}).\")\n",
    "    train, val, test = split_groups(groups, args.val, args.test, args.seed)\n",
    "    write_split(args.out, \"train\", train)\n",
    "    write_split(args.out, \"val\", val)\n",
    "    write_split(args.out, \"test\", test)\n",
    "    write_split(args.out, \"ground_truth_all\", train + val + test)\n",
    "    extract_charset(entries, args.out)\n",
    "    print(\"Fatto.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check on the generated splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "bash"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Check number of lines in each split and show first few entries from train\n",
    "wc -l splits_new/*.txt || true\n",
    "echo\n",
    "sed -n '1,5p' splits_new/train.txt || true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image preprocessing pipeline\n",
    "\n",
    "Ketos expects line images in a consistent format. We use ImageMagick (`convert`) to:\n",
    "- deskew lines,\n",
    "- convert to grayscale,\n",
    "- resize to a fixed height (64 px here),\n",
    "- center the line in a fixed canvas.\n",
    "\n",
    "The following shell snippet processes all line images and writes them into `processed/lines/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "bash"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Preprocess line images for ketos (deskew, grayscale, resize, center)\n",
    "mkdir -p processed/lines\n",
    "\n",
    "for f in 00_images/*.png; do\n",
    "    base=$(basename \"$f\")\n",
    "    convert \"$f\" \\\n",
    "        -deskew 40% \\\n",
    "        -colorspace Gray \\\n",
    "        -resize x64 \\\n",
    "        -background white -gravity center -extent 0x64 \\\n",
    "        \"processed/lines/$base\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing, we need to update the image paths in the split files to point to `processed/lines/` instead of `00_images/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "bash"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Update image paths in split files to use processed/lines instead of 00_images\n",
    "sed 's|^00_images/|processed/lines/|' splits_new/train.txt > splits_new/train_proc.txt\n",
    "sed 's|^00_images/|processed/lines/|' splits_new/val.txt   > splits_new/val_proc.txt\n",
    "sed 's|^00_images/|processed/lines/|' splits_new/test.txt  > splits_new/test_proc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extracting image lists and creating sidecar files\n",
    "\n",
    "Kraken/ketos training expects either:\n",
    "- split files with `image\\ttext`, or\n",
    "- image lists plus separate `.gt.txt` files per image.\n",
    "\n",
    "Here we generate:\n",
    "- simple image lists (one image path per line) for train/val/test;\n",
    "- sidecar `.gt.txt` files containing the transcription for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "bash"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Extract only image paths from the processed splits (train/val/test)\n",
    "awk -F'\\t' '{print $1}' splits_new/train_proc.txt > splits_new/train_images.txt\n",
    "awk -F'\\t' '{print $1}' splits_new/val_proc.txt   > splits_new/val_images.txt\n",
    "awk -F'\\t' '{print $1}' splits_new/test_proc.txt  > splits_new/test_images.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Inline script to create sidecar .gt.txt files from split files\n",
    "from pathlib import Path\n",
    "\n",
    "def write_sidecars(split_path: str) -> int:\n",
    "    p = Path(split_path)\n",
    "    created = 0\n",
    "    with p.open(encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            line = line.rstrip('\\n')\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                img, txt = line.split('\\t', 1)\n",
    "            except ValueError:\n",
    "                print(f\"[WARN] line {i} without TAB in {split_path}: {line[:120]}...\")\n",
    "                continue\n",
    "            img_path = Path(img)\n",
    "            if not img_path.exists():\n",
    "                print(f\"[WARN] missing image at line {i}: {img_path}\")\n",
    "                continue\n",
    "            sidecar = img_path.with_suffix('.gt.txt')  # e.g. foo.png -> foo.gt.txt\n",
    "            text = txt.strip('\\r\\n')\n",
    "            sidecar.write_text(text + '\\n', encoding='utf-8')\n",
    "            created += 1\n",
    "    return created\n",
    "\n",
    "total = 0\n",
    "for sp in ['splits_new/train_proc.txt', 'splits_new/val_proc.txt', 'splits_new/test_proc.txt']:\n",
    "    if Path(sp).exists():\n",
    "        c = write_sidecars(sp)\n",
    "        print(f\"[OK] {sp}: created {c} sidecar files\")\n",
    "        total += c\n",
    "print(\"Total new sidecars created:\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Verification: check that each image in the image lists has a corresponding .gt.txt sidecar\n",
    "from pathlib import Path\n",
    "\n",
    "missing = []\n",
    "for lst in ['splits_new/train_images.txt', 'splits_new/val_images.txt', 'splits_new/test_images.txt']:\n",
    "    p = Path(lst)\n",
    "    if not p.exists():\n",
    "        continue\n",
    "    for l in p.read_text(encoding='utf-8').splitlines():\n",
    "        img = Path(l.strip())\n",
    "        if not img.exists():\n",
    "            continue\n",
    "        gt = img.with_suffix('.gt.txt')\n",
    "        if not gt.exists():\n",
    "            missing.append(str(img))\n",
    "\n",
    "print(\"Total images without sidecar:\", len(missing))\n",
    "if missing[:10]:\n",
    "    print(\"First missing:\", missing[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the Kraken model with ketos\n",
    "\n",
    "Finally, we launch training using `ketos train`, starting from the pre‑trained\n",
    "Latin manuscript model `Tridis_Medieval_EarlyModern.mlmodel` and fine‑tuning it on our Italian dataset.\n",
    "\n",
    "> **Note:** This command is intended to be run in a shell inside the conda environment where Kraken/ketos are installed. Adjust paths and hyperparameters as needed for your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "bash"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Fine-tune Kraken model with ketos\n",
    "# Run this in a terminal within the environment where kraken/ketos is installed.\n",
    "ketos train \\\n",
    "    -f path \\\n",
    "    -i \"models/Tridis_Medieval_EarlyModern.mlmodel\" \\\n",
    "    --resize union \\\n",
    "    -q early \\\n",
    "    -N 40 \\\n",
    "    --min-epochs 5 \\\n",
    "    --lag 10 \\\n",
    "    -B 4 \\\n",
    "    -r 5e-5 \\\n",
    "    -o models/italian_finetuned.mlmodel_best.mlmodel \\\n",
    "    -t splits_new/train_images.txt \\\n",
    "    -e splits_new/val_images.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
